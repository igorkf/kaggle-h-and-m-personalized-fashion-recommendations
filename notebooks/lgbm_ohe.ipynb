{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import catboost\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from lightfm import LightFM\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from metric import apk, mapk\n",
    "from utils import plot_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_DIR = Path('artifacts')\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '100'\n",
    "\n",
    "transactions = pd.read_pickle(f\"input/{dataset}/transactions_train.pkl\")\n",
    "users = pd.read_pickle(f\"input/{dataset}/users.pkl\")\n",
    "items = pd.read_pickle(f\"input/{dataset}/items.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_type = 'LightGBM'\n",
    "    num_popular_items = 12\n",
    "    train_weeks = 3\n",
    "    repurchase_weeks = 123456\n",
    "    repurchase_num_items = 123456\n",
    "    item2item_weeks = 123456\n",
    "    item2item_num_items = 24\n",
    "    cooc_weeks = 12\n",
    "    cooc_threshold = 50\n",
    "    dynamic_feature_weeks = 8\n",
    "    volume_feature_weeks = 1\n",
    "\n",
    "# BEST\n",
    "# class CFG:\n",
    "#     model_type = 'CatBoost'\n",
    "#     num_popular_items = 12\n",
    "#     train_weeks = 3\n",
    "#     repurchase_weeks = 123456\n",
    "#     repurchase_num_items = 123456\n",
    "#     item2item_weeks = 123456\n",
    "#     item2item_num_items = 24\n",
    "#     cooc_weeks = 12\n",
    "#     cooc_threshold = 50\n",
    "#     dynamic_feature_weeks = 8\n",
    "#     volume_feature_weeks = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100% 1/1 [00:10<00:00, 10.49s/it]\n",
      "Epoch: 100% 1/1 [00:10<00:00, 10.06s/it]\n"
     ]
    }
   ],
   "source": [
    "def train_lfm(\n",
    "        transactions: pd.DataFrame,\n",
    "        n_user: int,\n",
    "        n_item: int,\n",
    "        week_start: int,\n",
    "        week_num: int,\n",
    "):\n",
    "    week_end = week_start + week_num\n",
    "    a = transactions.query(\"@week_start <= week < @week_end\")[['user', 'item']].drop_duplicates(ignore_index=True)\n",
    "\n",
    "    a_train = sparse.lil_matrix((n_user, n_item))\n",
    "    a_train[a['user'], a['item']] = 1\n",
    "\n",
    "    lightfm_params = {\n",
    "        'no_components': 16,\n",
    "        'learning_schedule': 'adadelta',\n",
    "        'loss': 'bpr',\n",
    "        'learning_rate': 0.005,\n",
    "    }\n",
    "\n",
    "    model = LightFM(**lightfm_params)\n",
    "    model.fit(a_train, epochs=1, num_threads=psutil.cpu_count(logical=False), verbose=True)\n",
    "    with open(ARTIFACTS_DIR / f\"lfm_{dataset}_{week_start}.pkl\", 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "train_lfm(transactions, len(users), len(items), CFG.train_weeks+1, CFG.repurchase_weeks)  # for train/valid\n",
    "train_lfm(transactions, len(users), len(items), CFG.train_weeks, CFG.repurchase_weeks)  # for train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidates(transactions: pd.DataFrame, target_users: np.ndarray, week: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    transactions\n",
    "        original transactions (user, item, week)\n",
    "    target_users, week\n",
    "        候補生成対象のユーザー\n",
    "        weekで指定されている週の段階での情報のみから作られる\n",
    "    \"\"\"\n",
    "    print(f\"week: {week}\")\n",
    "    assert len(target_users) == len(set(target_users))\n",
    "\n",
    "    def create_candidates_repurchase(\n",
    "            strategy: str,\n",
    "            transactions: pd.DataFrame,\n",
    "            target_users: np.ndarray,\n",
    "            week_start: int,\n",
    "            num_weeks: int,\n",
    "            max_items_per_user: int) -> pd.DataFrame:\n",
    "        week_end = week_start + num_weeks\n",
    "        tr = transactions.query(\"user in @target_users and @week_start <= week < @week_end\")[['user', 'item', 'week', 'day']].drop_duplicates(ignore_index=True)\n",
    "\n",
    "        gr_day = tr.groupby(['user', 'item'])['day'].min().reset_index(name='day')\n",
    "        gr_week = tr.groupby(['user', 'item'])['week'].min().reset_index(name='week')\n",
    "        gr_volume = tr.groupby(['user', 'item']).size().reset_index(name='volume')\n",
    "\n",
    "        gr_day['day_rank'] = gr_day.groupby('user')['day'].rank()\n",
    "        gr_week['week_rank'] = gr_week.groupby('user')['week'].rank()\n",
    "        gr_volume['volume_rank'] = gr_volume.groupby('user')['volume'].rank(ascending=False)\n",
    "\n",
    "        candidates = gr_day.merge(gr_week, on=['user', 'item']).merge(gr_volume, on=['user', 'item'])\n",
    "\n",
    "        candidates['rank_meta'] = 10**9 * candidates['day_rank'] + candidates['volume_rank']\n",
    "        candidates['rank_meta'] = candidates.groupby('user')['rank_meta'].rank(method='min')\n",
    "        # item2itemに使う場合は全件使うと無駄に重くなってしまうので削る\n",
    "        # dayの小ささ, volumeの大きさの辞書順にソートして上位アイテムのみ残す\n",
    "        # 全部残したい場合はmax_items_per_userに十分大きな数を指定する\n",
    "        candidates = candidates.query(\"rank_meta <= @max_items_per_user\").reset_index(drop=True)\n",
    "\n",
    "        candidates = candidates[['user', 'item', 'week_rank', 'volume_rank', 'rank_meta']].rename(columns={'week_rank': f'{strategy}_week_rank', 'volume_rank': f'{strategy}_volume_rank'})\n",
    "\n",
    "        candidates['strategy'] = strategy\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "    def create_candidates_popular(\n",
    "            strategy: str,\n",
    "            transactions: pd.DataFrame,\n",
    "            target_users: np.ndarray,\n",
    "            week_start: int,\n",
    "            week_num: int) -> pd.DataFrame:\n",
    "        week_end = week_start + week_num\n",
    "        tr = transactions.query(\"@week_start <= week < @week_end\")[['user', 'item']].drop_duplicates(ignore_index=True)\n",
    "        popular_items = tr['item'].value_counts().index.values[:CFG.num_popular_items]\n",
    "        popular_items = pd.DataFrame({\n",
    "            'item': popular_items,\n",
    "            'rank': range(CFG.num_popular_items),\n",
    "            'crossjoinkey': 1,\n",
    "        })\n",
    "\n",
    "        candidates = pd.DataFrame({\n",
    "            'user': target_users,\n",
    "            'crossjoinkey': 1,\n",
    "        })\n",
    "\n",
    "        candidates = candidates.merge(popular_items, on='crossjoinkey').drop('crossjoinkey', axis=1)\n",
    "        candidates = candidates.rename(columns={'rank': f'{strategy}_rank'})\n",
    "\n",
    "        candidates['strategy'] = strategy\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "    def create_candidates_cooc(\n",
    "            strategy: str,\n",
    "            transactions: pd.DataFrame,\n",
    "            week_start: int,\n",
    "            week_num: int,\n",
    "            base_candidates: pd.DataFrame) -> pd.DataFrame:\n",
    "        week_end = week_start + week_num\n",
    "        # TODO: 元々バグってたので閾値とか要検討\n",
    "        tr = transactions.query(\"@week_start <= week < @week_end\")[['user', 'item', 'week']].drop_duplicates(ignore_index=True)\n",
    "        tr = tr.merge(tr.rename(columns={'item': 'item_with', 'week': 'week_with'}), on='user').query(\"item != item_with and week <= week_with\")[['item', 'item_with']].reset_index(drop=True)\n",
    "        gr_sz = tr.groupby('item').size().reset_index(name='tot')\n",
    "        gr_cnt = tr.groupby(['item', 'item_with']).size().reset_index(name='cnt')\n",
    "        item2item = gr_cnt.merge(gr_sz, on='item')\n",
    "        item2item['ratio'] = item2item['cnt'] / item2item['tot']\n",
    "        item2item = item2item.query(\"cnt > @CFG.cooc_threshold\").reset_index(drop=True)\n",
    "\n",
    "        candidates = base_candidates.merge(item2item, on='item').drop(['item', 'cnt'], axis=1).rename(columns={'item_with': 'item'})\n",
    "        base_candidates_columns = [c for c in base_candidates.columns if '_' in c]\n",
    "        base_candidates_replace = {c: f\"{strategy}_{c}\" for c in base_candidates_columns}\n",
    "        candidates = candidates.rename(columns=base_candidates_replace)\n",
    "        candidates = candidates.rename(columns={'ratio': f'{strategy}_ratio', 'tot': f'{strategy}_tot'})\n",
    "\n",
    "        candidates['strategy'] = strategy\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    def create_candidates_same_product_code(\n",
    "            strategy: str,\n",
    "            items: pd.DataFrame,\n",
    "            base_candidates: pd.DataFrame) -> pd.DataFrame:\n",
    "        item2item = items[['item', 'product_code']].merge(items[['item', 'product_code']].rename({'item': 'item_with'}, axis=1), on='product_code')[['item', 'item_with']].query(\"item != item_with\").reset_index(drop=True)\n",
    "\n",
    "        candidates = base_candidates.merge(item2item, on='item').drop('item', axis=1).rename(columns={'item_with': 'item'})\n",
    "\n",
    "        candidates['min_rank_meta'] = candidates.groupby(['user', 'item'])['rank_meta'].transform('min')\n",
    "        candidates = candidates.query(\"rank_meta == min_rank_meta\").reset_index(drop=True)\n",
    "        \n",
    "        base_candidates_columns = [c for c in base_candidates.columns if '_' in c]\n",
    "        base_candidates_replace = {c: f\"{strategy}_{c}\" for c in base_candidates_columns}\n",
    "        candidates = candidates.rename(columns=base_candidates_replace)\n",
    "\n",
    "        candidates['strategy'] = strategy\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    candidates_repurchase = create_candidates_repurchase('repurchase', transactions, target_users, week, CFG.repurchase_weeks, CFG.repurchase_num_items)\n",
    "    candidates_popular = create_candidates_popular('pop', transactions, target_users, week, 1)\n",
    "\n",
    "    candidates_item2item = create_candidates_repurchase('item2item', transactions, target_users, week, CFG.repurchase_weeks, CFG.item2item_num_items)\n",
    "    candidates_cooc = create_candidates_cooc('cooc', transactions, week, CFG.cooc_weeks, candidates_item2item)\n",
    "    candidates_same_product_code = create_candidates_same_product_code('same_product_code', items, candidates_item2item)\n",
    "\n",
    "    def drop_common_user_item(candidates_target: pd.DataFrame, candidates_reference: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        candidates_targetのうちuser, itemの組がcandidates_referenceにあるものを落とす\n",
    "        \"\"\"\n",
    "        tmp = candidates_reference[['user', 'item']].reset_index(drop=True)\n",
    "        tmp['flag'] = 1\n",
    "        candidates = candidates_target.merge(tmp, on=['user', 'item'], how='left')\n",
    "        return candidates.query(\"flag != 1\").reset_index(drop=True).drop('flag', axis=1)\n",
    "\n",
    "    candidates_cooc = drop_common_user_item(candidates_cooc, candidates_repurchase)\n",
    "    candidates_same_product_code = drop_common_user_item(candidates_same_product_code, candidates_repurchase)\n",
    "\n",
    "    candidates = [\n",
    "        candidates_repurchase,\n",
    "        candidates_popular,\n",
    "        candidates_cooc,\n",
    "        candidates_same_product_code,\n",
    "    ]\n",
    "    candidates = pd.concat(candidates)\n",
    "\n",
    "    print(f\"volume: {len(candidates)}\")\n",
    "    print(f\"duplicates: {len(candidates) / len(candidates[['user', 'item']].drop_duplicates())}\")\n",
    "\n",
    "    volumes = candidates.groupby('strategy').size().reset_index(name='volume').sort_values(by='volume', ascending=False).reset_index(drop=True)\n",
    "    volumes['ratio'] = volumes['volume'] / volumes['volume'].sum()\n",
    "    print(volumes)\n",
    "\n",
    "    meta_columns = [c for c in candidates.columns if c.endswith('_meta')]\n",
    "    return candidates.drop(meta_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week: 1\n",
      "volume: 12354253\n",
      "duplicates: 1.0596052180843134\n",
      "            strategy   volume     ratio\n",
      "0  same_product_code  6876142  0.556581\n",
      "1         repurchase  3446562  0.278978\n",
      "2               cooc  1203741  0.097435\n",
      "3                pop   827808  0.067006\n",
      "week: 2\n",
      "volume: 13148159\n",
      "duplicates: 1.0663447719892427\n",
      "            strategy   volume     ratio\n",
      "0  same_product_code  7228749  0.549792\n",
      "1         repurchase  3637145  0.276628\n",
      "2               cooc  1418037  0.107851\n",
      "3                pop   864228  0.065730\n",
      "week: 3\n",
      "volume: 13922204\n",
      "duplicates: 1.0692281686575584\n",
      "            strategy   volume     ratio\n",
      "0  same_product_code  7678831  0.551553\n",
      "1         repurchase  3742824  0.268838\n",
      "2               cooc  1590685  0.114255\n",
      "3                pop   909864  0.065353\n",
      "week: 4\n",
      "volume: 14853722\n",
      "duplicates: 1.0730213586103197\n",
      "            strategy   volume     ratio\n",
      "0  same_product_code  8177755  0.550553\n",
      "1         repurchase  3915353  0.263594\n",
      "2               cooc  1797578  0.121019\n",
      "3                pop   963036  0.064835\n"
     ]
    }
   ],
   "source": [
    "# valid: week=0\n",
    "# train: week=1..CFG.train_weeks\n",
    "candidates = []\n",
    "for week in range(1+CFG.train_weeks):\n",
    "    target_users = transactions.query(\"week == @week\")['user'].unique()\n",
    "    candidates.append(create_candidates(transactions, target_users, week+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_labels(candidates: pd.DataFrame, week: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    candidatesに対してweekで指定される週のトランザクションからラベルを付与する\n",
    "    \"\"\"\n",
    "    print(f\"week: {week}\")\n",
    "    labels = transactions[transactions['week'] == week][['user', 'item']].drop_duplicates(ignore_index=True)\n",
    "    labels['y'] = 1\n",
    "    original_positives = len(labels)\n",
    "    labels = candidates.merge(labels, on=['user', 'item'], how='left')\n",
    "    labels['y'] = labels['y'].fillna(0)\n",
    "\n",
    "    remaining_positives_total = labels[['user', 'item', 'y']].drop_duplicates(ignore_index=True)['y'].sum()\n",
    "    recall = remaining_positives_total / original_positives\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "    volumes = candidates.groupby('strategy').size().reset_index(name='volume')\n",
    "    remaining_positives = labels.groupby('strategy')['y'].sum().reset_index()\n",
    "    remaining_positives = remaining_positives.merge(volumes, on='strategy')\n",
    "    remaining_positives['recall'] = remaining_positives['y'] / original_positives\n",
    "    remaining_positives['hit_ratio'] = remaining_positives['y'] / remaining_positives['volume']\n",
    "    remaining_positives = remaining_positives.sort_values(by='y', ascending=False).reset_index(drop=True)\n",
    "    print(remaining_positives)\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week: 0\n",
      "Recall: 0.10367382841742775\n",
      "            strategy       y   volume    recall  hit_ratio\n",
      "0         repurchase  8437.0  3446562  0.039475   0.002448\n",
      "1  same_product_code  7888.0  6876142  0.036907   0.001147\n",
      "2               cooc  5096.0  1203741  0.023843   0.004233\n",
      "3                pop  4833.0   827808  0.022613   0.005838\n",
      "week: 1\n",
      "Recall: 0.09945592558466061\n",
      "            strategy       y   volume    recall  hit_ratio\n",
      "0         repurchase  8955.0  3637145  0.039292   0.002462\n",
      "1  same_product_code  8554.0  7228749  0.037532   0.001183\n",
      "2               cooc  5210.0  1418037  0.022860   0.003674\n",
      "3                pop  4539.0   864228  0.019916   0.005252\n",
      "week: 2\n",
      "Recall: 0.101040012769139\n",
      "            strategy       y   volume    recall  hit_ratio\n",
      "0         repurchase  9195.0  3742824  0.038622   0.002457\n",
      "1  same_product_code  8829.0  7678831  0.037085   0.001150\n",
      "2               cooc  5376.0  1590685  0.022581   0.003380\n",
      "3                pop  5342.0   909864  0.022438   0.005871\n",
      "week: 3\n",
      "Recall: 0.09409339582712836\n",
      "            strategy       y   volume    recall  hit_ratio\n",
      "0  same_product_code  9328.0  8177755  0.036556   0.001141\n",
      "1         repurchase  9002.0  3915353  0.035278   0.002299\n",
      "2               cooc  5609.0  1797578  0.021981   0.003120\n",
      "3                pop  4984.0   963036  0.019532   0.005175\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(candidates)):\n",
    "    candidates[idx] = merge_labels(candidates[idx], idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop trivial queries: 12354253 -> 3469506\n",
      "drop trivial queries: 13148159 -> 3681372\n",
      "drop trivial queries: 13922204 -> 3919804\n",
      "drop trivial queries: 14853722 -> 3965310\n"
     ]
    }
   ],
   "source": [
    "def drop_trivial_users(labels):\n",
    "    \"\"\"\n",
    "    LightGBMのxendgcやlambdarankでは正例のみや負例のみのuserは学習に無意味なのと、メトリックの計算がおかしくなるので省く\n",
    "    \"\"\"\n",
    "    bef = len(labels)\n",
    "    df = labels[labels['user'].isin(labels[['user', 'y']].drop_duplicates().groupby('user').size().reset_index(name='sz').query(\"sz==2\").user)].reset_index(drop=True)\n",
    "    aft = len(df)\n",
    "    print(f\"drop trivial queries: {bef} -> {aft}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "for idx in range(len(candidates)):\n",
    "    candidates[idx]['week'] = idx\n",
    "\n",
    "valid_all = candidates[0].copy()\n",
    "\n",
    "for idx in range(len(candidates)):\n",
    "    candidates[idx] = drop_trivial_users(candidates[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week: 1\n",
      "week: 1\n",
      "week: 2\n",
      "week: 3\n",
      "week: 4\n"
     ]
    }
   ],
   "source": [
    "def attach_features(transactions: pd.DataFrame, users: pd.DataFrame, items: pd.DataFrame, candidates: pd.DataFrame, week: int, pretrain_week: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    user, itemに対して特徴を横付けする\n",
    "    week: これを含めた以前の情報は使って良い\n",
    "    \"\"\"\n",
    "    print(f\"week: {week}\")\n",
    "    n_original = len(candidates)\n",
    "    df = candidates.copy()\n",
    "\n",
    "    # user static features\n",
    "    user_features = ['FN', 'Active', 'age', 'club_member_status_idx', 'fashion_news_frequency_idx']\n",
    "    df = df.merge(users[['user'] + user_features], on='user')\n",
    "\n",
    "    # item static features\n",
    "    item_features = [c for c in items.columns if c.endswith('idx')]\n",
    "    df = df.merge(items[['item'] + item_features], on='item')\n",
    "\n",
    "    # user dynamic features (transactions)\n",
    "    week_end = week + CFG.dynamic_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('user')[['price', 'sales_channel_id']].agg(['mean', 'std'])\n",
    "    tmp.columns = ['user_' + '_'.join(a) for a in tmp.columns.to_flat_index()]\n",
    "    df = df.merge(tmp, on='user', how='left')\n",
    "\n",
    "    # item dynamic features (transactions)\n",
    "    week_end = week + CFG.dynamic_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('item')[['price', 'sales_channel_id']].agg(['mean', 'std'])\n",
    "    tmp.columns = ['item_' + '_'.join(a) for a in tmp.columns.to_flat_index()]\n",
    "    df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    # item dynamic features (user features)\n",
    "    week_end = week + CFG.dynamic_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").merge(users[['user', 'age']], on='user')\n",
    "    tmp = tmp.groupby('item')['age'].agg(['mean', 'std'])\n",
    "    tmp.columns = [f'age_{a}' for a in tmp.columns.to_flat_index()]\n",
    "    df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    # item freshness features\n",
    "    tmp = transactions.query(\"@week <= week\").groupby('item')['day'].min().reset_index(name='item_day_min')\n",
    "    tmp['item_day_min'] -= transactions.query(\"@week == week\")['day'].min()\n",
    "    df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    # item volume features\n",
    "    week_end = week + CFG.volume_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('item').size().reset_index(name='item_volume')\n",
    "    df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    # # user freshness features\n",
    "    # tmp = transactions.query(\"@week <= week\").groupby('user')['day'].min().reset_index(name='user_day_min')\n",
    "    # tmp['user_day_min'] -= transactions.query(\"@week == week\")['day'].min()\n",
    "    # df = df.merge(tmp, on='user', how='left')\n",
    "\n",
    "    # # user volume features\n",
    "    # week_end = week + CFG.volume_feature_weeks\n",
    "    # tmp = transactions.query(\"@week <= week < @week_end\").groupby('user').size().reset_index(name='user_volume')\n",
    "    # df = df.merge(tmp, on='user', how='left')\n",
    "\n",
    "    # user-item freshness features\n",
    "    tmp = transactions.query(\"@week <= week\").groupby(['user', 'item'])['day'].min().reset_index(name='user_item_day_min')\n",
    "    tmp['user_item_day_min'] -= transactions.query(\"@week == week\")['day'].min()\n",
    "\n",
    "    df = df.merge(tmp, on=['item', 'user'], how='left')\n",
    "\n",
    "    # user-item volume features\n",
    "    week_end = week + CFG.volume_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby(['user', 'item']).size().reset_index(name='user_item_volume')\n",
    "    df = df.merge(tmp, on=['user', 'item'], how='left')\n",
    "\n",
    "    # lfm features\n",
    "    lfm_path = ARTIFACTS_DIR / f\"lfm_{dataset}_{pretrain_week}.pkl\"\n",
    "    unseen_users = sorted(set(users['user']) - set(transactions.query(\"week >= @pretrain_week\")['user']))\n",
    "\n",
    "    with open(lfm_path, 'rb') as f:\n",
    "        lfm = pickle.load(f)\n",
    "    no_components = lfm.get_params()['no_components']\n",
    "\n",
    "    # df['lfm'] = lfm.predict(df['user'].values, df['item'].values, num_threads=psutil.cpu_count(logical=False))\n",
    "\n",
    "    user_reps = np.hstack([lfm.user_embeddings, lfm.user_biases.reshape((len(users), 1))])\n",
    "    user_reps[unseen_users, :] = np.nan\n",
    "    user_reps = pd.DataFrame(user_reps, columns=[f'user_rep_{i}' for i in range(no_components+1)])\n",
    "    user_reps = pd.concat([pd.DataFrame({'user': range(len(users))}), user_reps], axis=1)\n",
    "    df = df.merge(user_reps, on='user')\n",
    "\n",
    "    assert len(df) == n_original\n",
    "    return df\n",
    "\n",
    "\n",
    "valid_all = attach_features(transactions, users, items, valid_all, 1, CFG.train_weeks+1)\n",
    "# pretrained modelの学習期間が評価時と提出時で異なるので、candidatesは残しておく\n",
    "datasets = [attach_features(transactions, users, items, candidates[idx], 1+idx, CFG.train_weeks+1) for idx in range(len(candidates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(datasets)):\n",
    "    datasets[idx]['query_group'] = datasets[idx]['week'].astype(str) + '_' + datasets[idx]['user'].astype(str)\n",
    "    datasets[idx] = datasets[idx].sort_values(by='query_group').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_train(datasets, begin, num):\n",
    "    train = pd.concat([datasets[idx] for idx in range(begin, begin+num)])\n",
    "    return train\n",
    "\n",
    "valid = datasets[0]\n",
    "train = concat_train(datasets, 1, CFG.train_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'item', 'repurchase_week_rank', 'repurchase_volume_rank', 'pop_rank', 'cooc_item2item_week_rank', 'cooc_item2item_volume_rank', 'cooc_tot', 'cooc_ratio', 'same_product_code_item2item_week_rank', 'same_product_code_item2item_volume_rank', 'FN', 'Active', 'age', 'club_member_status_idx', 'fashion_news_frequency_idx', 'product_type_no_idx', 'product_group_name_idx', 'graphical_appearance_no_idx', 'colour_group_code_idx', 'perceived_colour_value_id_idx', 'perceived_colour_master_id_idx', 'department_no_idx', 'index_code_idx', 'index_group_no_idx', 'section_no_idx', 'garment_group_no_idx', 'user_price_mean', 'user_price_std', 'user_sales_channel_id_mean', 'user_sales_channel_id_std', 'item_price_mean', 'item_price_std', 'item_sales_channel_id_mean', 'item_sales_channel_id_std', 'age_mean', 'age_std', 'item_day_min', 'item_volume', 'user_item_day_min', 'user_item_volume', 'user_rep_0', 'user_rep_1', 'user_rep_2', 'user_rep_3', 'user_rep_4', 'user_rep_5', 'user_rep_6', 'user_rep_7', 'user_rep_8', 'user_rep_9', 'user_rep_10', 'user_rep_11', 'user_rep_12', 'user_rep_13', 'user_rep_14', 'user_rep_15', 'user_rep_16']\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [c for c in valid.columns if c not in ['y', 'strategy', 'query_group', 'week']]\n",
    "print(feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['club_member_status_idx', 'fashion_news_frequency_idx', 'product_type_no_idx', 'product_group_name_idx', 'graphical_appearance_no_idx', 'colour_group_code_idx', 'perceived_colour_value_id_idx', 'perceived_colour_master_id_idx', 'department_no_idx', 'index_code_idx', 'index_group_no_idx', 'section_no_idx', 'garment_group_no_idx'] [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cat_feature_values = [c for c in feature_columns if c.endswith('idx')]\n",
    "cat_features = [feature_columns.index(c) for c in cat_feature_values]\n",
    "print(cat_feature_values, cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_group(df):\n",
    "    def run_length_encoding(sequence):\n",
    "        comp_seq_index, = np.concatenate(([True], sequence[1:] != sequence[:-1], [True])).nonzero()\n",
    "        return sequence[comp_seq_index[:-1]], np.ediff1d(comp_seq_index)\n",
    "    users = df['user'].values\n",
    "    _, group = run_length_encoding(users)\n",
    "    return list(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=True)\n",
    "ohe.fit(train.append(valid)[cat_feature_values])\n",
    "\n",
    "def get_ohe(df):\n",
    "    cont_features = list(set(feature_columns).difference(cat_feature_values))\n",
    "    X_cont = sparse.coo_matrix(df[cont_features])\n",
    "    X_cat = ohe.transform(df[cat_feature_values])\n",
    "    return sparse.hstack([X_cont, X_cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total groups: 51203, total data: 11566486\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.405107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10368\n",
      "[LightGBM] [Info] Number of data points in the train set: 11566486, number of used features: 58\n",
      "[LightGBM] [Info] Total groups: 15871, total data: 3469506\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    }
   ],
   "source": [
    "if CFG.model_type == 'LightGBM':\n",
    "    group_train = get_query_group(train)\n",
    "    group_valid = get_query_group(valid)\n",
    "\n",
    "#     train_ohe = get_ohe(train[feature_columns])\n",
    "#     valid_ohe = get_ohe(valid[feature_columns])\n",
    "\n",
    "#     train_dataset = lgb.Dataset(train_ohe, train['y'], group=group_train)\n",
    "#     valid_dataset = lgb.Dataset(valid_ohe, valid['y'], group=group_valid, reference=train_dataset)\n",
    "\n",
    "    train_dataset = lgb.Dataset(train[feature_columns], train['y'], group=group_train)\n",
    "    valid_dataset = lgb.Dataset(valid[feature_columns], valid['y'], group=group_valid, reference=train_dataset)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'xendcg',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 1e-6,\n",
    "        'num_leaves': 255,\n",
    "        'min_data_in_leaf': 100,\n",
    "        'metric': 'map',\n",
    "        'eval_at': 12,\n",
    "    }\n",
    "\n",
    "    model = lgb.train(params, train_dataset, valid_sets=[train_dataset, valid_dataset], num_boost_round=1000, callbacks=[lgb.early_stopping(20)])\n",
    "\n",
    "    lgb.plot_importance(model, importance_type='gain', figsize=(8, 16))\n",
    "\n",
    "elif CFG.model_type == 'CatBoost':\n",
    "    train_pool = catboost.Pool(data=train[feature_columns], label=train['y'], group_id=train['query_group'], cat_features=cat_features)\n",
    "    valid_pool = catboost.Pool(data=valid[feature_columns], label=valid['y'], group_id=valid['query_group'], cat_features=cat_features)\n",
    "\n",
    "    params = {\n",
    "        'loss_function': 'YetiRank',\n",
    "        'use_best_model': True,\n",
    "        'one_hot_max_size': 300,\n",
    "    }\n",
    "    model = catboost.CatBoost(params)\n",
    "    model.fit(train_pool, eval_set=valid_pool)\n",
    "\n",
    "    feature_importance = model.get_feature_importance(train_pool)\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    plt.yticks(range(len(feature_columns)), np.array(feature_columns)[sorted_idx])\n",
    "    plt.barh(range(len(feature_columns)), feature_importance[sorted_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.model_type == 'LightGBM':\n",
    "    # valid_all_ohe = get_ohe(valid_all[feature_columns])\n",
    "    # valid_all['pred'] = model.predict(valid_all_ohe)\n",
    "    valid_all['pred'] = model.predict(valid_all[feature_columns])\n",
    "else:\n",
    "    valid_all['pred'] = model.predict(valid_all[feature_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = valid_all.groupby(['user', 'item'])['pred'].max().reset_index()\n",
    "pred = pred.sort_values(by=['user', 'pred'], ascending=False).reset_index(drop=True).groupby('user')['item'].apply(lambda x: list(x)[:12]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = transactions.query(\"week == 0\").groupby('user')['item'].apply(list).reset_index().rename(columns={'item': 'gt'})\n",
    "merged = gt.merge(pred, on='user', how='left')\n",
    "merged['item'] = merged['item'].fillna('').apply(list)\n",
    "\n",
    "print('mAP@12:', mapk(merged['gt'], merged['item']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_222/2103537015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [attach_features(transactions, users, items, candidates[idx], 1+idx, CFG.train_weeks) for idx in range(len(candidates))]\n",
    "\n",
    "for idx in range(len(datasets)):\n",
    "    datasets[idx]['query_group'] = datasets[idx]['week'].astype(str) + '_' + datasets[idx]['user'].astype(str)\n",
    "    datasets[idx] = datasets[idx].sort_values(by='query_group').reset_index(drop=True)\n",
    "\n",
    "train = concat_train(datasets, 0, CFG.train_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.model_type == 'LightGBM':\n",
    "    group_train = get_query_group(train)\n",
    "    train_ohe = get_ohe(train[feature_columns])\n",
    "    train_dataset = lgb.Dataset(train_ohe, train['y'], group=group_train)\n",
    "\n",
    "    best_iteration = model.best_iteration\n",
    "    model = lgb.train(params, train_dataset, num_boost_round=best_iteration)\n",
    "\n",
    "    lgb.plot_importance(model, importance_type='gain', figsize=(8, 16))\n",
    "\n",
    "elif CFG.model_type == 'CatBoost':\n",
    "    train_pool = catboost.Pool(data=train[feature_columns], label=train['y'], group_id=train['query_group'], cat_features=cat_features)\n",
    "\n",
    "    params['iterations'] = model.get_best_iteration()\n",
    "    params['use_best_model'] = False\n",
    "    model = catboost.CatBoost(params)\n",
    "    model.fit(train_pool)\n",
    "\n",
    "    feature_importance = model.get_feature_importance(train_pool)\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    plt.yticks(range(len(feature_columns)), np.array(feature_columns)[sorted_idx])\n",
    "    plt.barh(range(len(feature_columns)), feature_importance[sorted_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = users['user'].values\n",
    "all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid OOM\n",
    "preds = []\n",
    "\n",
    "n_split_prediction = 10\n",
    "n_chunk = (len(all_users) + n_split_prediction - 1)// n_split_prediction\n",
    "for i in range(0, len(all_users), n_chunk):\n",
    "    target_users = all_users[i:i+n_chunk]\n",
    "\n",
    "    candidates = create_candidates(transactions, target_users, 0)\n",
    "    candidates = attach_features(transactions, users, items, candidates, 0, CFG.train_weeks)\n",
    "    \n",
    "    if CFG.model_type == 'LightGBM':\n",
    "        candidates_ohe = get_ohe(candidates[feature_columns])\n",
    "        candidates['pred'] = model.predict(candidates_ohe)\n",
    "    else:\n",
    "        candidates['pred'] = model.predict(candidates[feature_columns])\n",
    "    pred = candidates.groupby(['user', 'item'])['pred'].max().reset_index()\n",
    "    pred = pred.sort_values(by=['user', 'pred'], ascending=False).reset_index(drop=True).groupby('user')['item'].apply(lambda x: list(x)[:12]).reset_index()\n",
    "    preds.append(pred)\n",
    "\n",
    "pred = pd.concat(preds).reset_index(drop=True)\n",
    "assert len(pred) == len(all_users)\n",
    "assert np.array_equal(pred['user'].values, all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plot_images(dataset, pred['item'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_user = pd.read_pickle(f\"input/{dataset}/mp_customer_id.pkl\")\n",
    "mp_item = pd.read_pickle(f\"input/{dataset}/mp_article_id.pkl\")\n",
    "\n",
    "a_user = mp_user['val'].values\n",
    "a_item = mp_item['val'].values\n",
    "\n",
    "pred['customer_id'] = pred['user'].apply(lambda x: a_user[x])\n",
    "pred['prediction'] = pred['item'].apply(lambda x: list(map(lambda y: a_item[y], x)))\n",
    "\n",
    "pred['prediction'] = pred['prediction'].apply(lambda x: ' '.join(map(str, x)))\n",
    "\n",
    "submission = pred[['customer_id', 'prediction']]\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c89b5301ada1d96cd3523f144e4a3cd3ad36f6698a61e6b8277fb627756a86c6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
