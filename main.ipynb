{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import catboost\n",
    "import faiss\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from lfm import calc_embeddings, calc_scores\n",
    "from metric import apk, mapk\n",
    "from utils import plot_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    start_time = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] {time.time() - start_time:.3f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '100'\n",
    "\n",
    "transactions = pd.read_pickle(f\"input/{dataset}/transactions_train.pkl\")\n",
    "users = pd.read_pickle(f\"input/{dataset}/users.pkl\")\n",
    "items = pd.read_pickle(f\"input/{dataset}/items.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CFG:\n",
    "#     model_type = 'LightGBM'\n",
    "#     popular_num_items = 12\n",
    "#     popular_days = 7\n",
    "#     train_weeks = 1\n",
    "#     item2item_num_items = 24\n",
    "#     cooc_weeks = 12\n",
    "#     cooc_threshold = 50\n",
    "#     dynamic_feature_weeks = 8\n",
    "#     volume_feature_days = 7\n",
    "\n",
    "\n",
    "# BEST\n",
    "class CFG:\n",
    "    model_type = 'CatBoost'\n",
    "    popular_num_items = 36\n",
    "    popular_weeks = 1\n",
    "    train_weeks = 3\n",
    "    item2item_num_items = 24\n",
    "    cooc_weeks = 12\n",
    "    cooc_threshold = 50\n",
    "    ohe_distance_num_items = 32\n",
    "    ohe_distance_num_weeks = 1\n",
    "    dynamic_feature_weeks = 8\n",
    "    volume_feature_weeks = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidates(transactions: pd.DataFrame, target_users: np.ndarray, week: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    transactions\n",
    "        original transactions (user, item, week)\n",
    "    target_users, week\n",
    "        候補生成対象のユーザー\n",
    "        weekで指定されている週の段階での情報のみから作られる\n",
    "    \"\"\"\n",
    "    print(f\"create candidates (week: {week})\")\n",
    "    assert len(target_users) == len(set(target_users))\n",
    "\n",
    "    def create_candidates_repurchase(\n",
    "            strategy: str,\n",
    "            transactions: pd.DataFrame,\n",
    "            target_users: np.ndarray,\n",
    "            week_start: int,\n",
    "            max_items_per_user: int=1234567890\n",
    "        ) -> pd.DataFrame:\n",
    "        tr = transactions.query(\"user in @target_users and @week_start <= week\")[['user', 'item', 'week', 'day']].drop_duplicates(ignore_index=True)\n",
    "\n",
    "        gr_day = tr.groupby(['user', 'item'])['day'].min().reset_index(name='day')\n",
    "        gr_week = tr.groupby(['user', 'item'])['week'].min().reset_index(name='week')\n",
    "        gr_volume = tr.groupby(['user', 'item']).size().reset_index(name='volume')\n",
    "\n",
    "        gr_day['day_rank'] = gr_day.groupby('user')['day'].rank()\n",
    "        gr_week['week_rank'] = gr_week.groupby('user')['week'].rank()\n",
    "        gr_volume['volume_rank'] = gr_volume.groupby('user')['volume'].rank(ascending=False)\n",
    "\n",
    "        candidates = gr_day.merge(gr_week, on=['user', 'item']).merge(gr_volume, on=['user', 'item'])\n",
    "\n",
    "        candidates['rank_meta'] = 10**9 * candidates['day_rank'] + candidates['volume_rank']\n",
    "        candidates['rank_meta'] = candidates.groupby('user')['rank_meta'].rank(method='min')\n",
    "        # item2itemに使う場合は全件使うと無駄に重くなってしまうので削る\n",
    "        # dayの小ささ, volumeの大きさの辞書順にソートして上位アイテムのみ残す\n",
    "        # 全部残したい場合はmax_items_per_userに十分大きな数を指定する\n",
    "        candidates = candidates.query(\"rank_meta <= @max_items_per_user\").reset_index(drop=True)\n",
    "\n",
    "        candidates = candidates[['user', 'item', 'week_rank', 'volume_rank', 'rank_meta']].rename(columns={'week_rank': f'{strategy}_week_rank', 'volume_rank': f'{strategy}_volume_rank'})\n",
    "\n",
    "        candidates['strategy'] = strategy\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "    def create_candidates_popular(\n",
    "            transactions: pd.DataFrame,\n",
    "            target_users: np.ndarray,\n",
    "            week_start: int,\n",
    "            num_weeks: int,\n",
    "            num_items: int,\n",
    "        ) -> pd.DataFrame:\n",
    "        tr = transactions.query(\"@week_start <= week < @week_start + @num_weeks\")[['user', 'item']].drop_duplicates(ignore_index=True)\n",
    "        popular_items = tr['item'].value_counts().index.values[:num_items]\n",
    "        popular_items = pd.DataFrame({\n",
    "            'item': popular_items,\n",
    "            'rank': range(num_items),\n",
    "            'crossjoinkey': 1,\n",
    "        })\n",
    "\n",
    "        candidates = pd.DataFrame({\n",
    "            'user': target_users,\n",
    "            'crossjoinkey': 1,\n",
    "        })\n",
    "\n",
    "        candidates = candidates.merge(popular_items, on='crossjoinkey').drop('crossjoinkey', axis=1)\n",
    "        candidates = candidates.rename(columns={'rank': f'pop_rank'})\n",
    "\n",
    "        candidates['strategy'] = 'pop'\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "    def create_candidates_cooc(\n",
    "            transactions: pd.DataFrame,\n",
    "            base_candidates: pd.DataFrame,\n",
    "            week_start: int,\n",
    "            num_weeks: int,\n",
    "            pair_count_threshold: int,\n",
    "        ) -> pd.DataFrame:\n",
    "        week_end = week_start + num_weeks\n",
    "        tr = transactions.query(\"@week_start <= week < @week_end\")[['user', 'item', 'week']].drop_duplicates(ignore_index=True)\n",
    "        tr = tr.merge(tr.rename(columns={'item': 'item_with', 'week': 'week_with'}), on='user').query(\"item != item_with and week <= week_with\")[['item', 'item_with']].reset_index(drop=True)\n",
    "        gr_item_count = tr.groupby('item').size().reset_index(name='item_count')\n",
    "        gr_pair_count = tr.groupby(['item', 'item_with']).size().reset_index(name='pair_count')\n",
    "        item2item = gr_pair_count.merge(gr_item_count, on='item')\n",
    "        item2item['ratio'] = item2item['pair_count'] / item2item['item_count']\n",
    "        item2item = item2item.query(\"pair_count > @pair_count_threshold\").reset_index(drop=True)\n",
    "\n",
    "        candidates = base_candidates.merge(item2item, on='item').drop(['item', 'pair_count'], axis=1).rename(columns={'item_with': 'item'})\n",
    "        base_candidates_columns = [c for c in base_candidates.columns if '_' in c]\n",
    "        base_candidates_replace = {c: f\"cooc_{c}\" for c in base_candidates_columns}\n",
    "        candidates = candidates.rename(columns=base_candidates_replace)\n",
    "        candidates = candidates.rename(columns={'ratio': 'cooc_ratio', 'item_count': f'cooc_item_count'})\n",
    "\n",
    "        candidates['strategy'] = 'cooc'\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "    def create_candidates_same_product_code(\n",
    "            items: pd.DataFrame,\n",
    "            base_candidates: pd.DataFrame\n",
    "        ) -> pd.DataFrame:\n",
    "        item2item = items[['item', 'product_code']].merge(items[['item', 'product_code']].rename({'item': 'item_with'}, axis=1), on='product_code')[['item', 'item_with']].query(\"item != item_with\").reset_index(drop=True)\n",
    "\n",
    "        candidates = base_candidates.merge(item2item, on='item').drop('item', axis=1).rename(columns={'item_with': 'item'})\n",
    "\n",
    "        candidates['min_rank_meta'] = candidates.groupby(['user', 'item'])['rank_meta'].transform('min')\n",
    "        candidates = candidates.query(\"rank_meta == min_rank_meta\").reset_index(drop=True)\n",
    "        \n",
    "        base_candidates_columns = [c for c in base_candidates.columns if '_' in c]\n",
    "        base_candidates_replace = {c: f\"same_product_code_{c}\" for c in base_candidates_columns}\n",
    "        candidates = candidates.rename(columns=base_candidates_replace)\n",
    "\n",
    "        candidates['strategy'] = 'same_product_code'\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "    def create_candidates_ohe_distance(\n",
    "        transactions: pd.DataFrame,\n",
    "        users: pd.DataFrame,\n",
    "        items: pd.DataFrame,\n",
    "        target_users: np.ndarray,\n",
    "        week_start: int,\n",
    "        num_weeks: int,\n",
    "        num_items: int,\n",
    "    ) -> pd.DataFrame:\n",
    "        users_with_ohe = users[['user']].query(\"user in @target_users\")\n",
    "        cols = [c for c in items.columns if c.endswith('_idx')]\n",
    "        for c in cols:\n",
    "            tmp = pd.read_pickle(f\"artifacts/user_features/user_ohe_agg_dataset{dataset}_week{week}_{c}.pkl\")\n",
    "            cs = [c for c in tmp.columns if c.endswith('_mean')]\n",
    "            users_with_ohe = users_with_ohe.merge(tmp[['user'] + cs], on='user')\n",
    "\n",
    "        users_with_ohe = users_with_ohe.dropna().reset_index(drop=True)\n",
    "        limited_users = users_with_ohe['user'].values\n",
    "\n",
    "        recent_items = transactions.query(\"@week <= week < @week + @num_weeks\")['item'].unique()\n",
    "        items_with_ohe = pd.get_dummies(items[['item'] + cols], columns=cols)\n",
    "        items_with_ohe = items_with_ohe.query(\"item in @recent_items\").reset_index(drop=True)\n",
    "        limited_items = items_with_ohe['item'].values\n",
    "\n",
    "        item_cols = [c for c in items_with_ohe.columns if c != 'item']\n",
    "        user_cols = [f'user_ohe_agg_{c}_mean' for c in item_cols]\n",
    "        users_with_ohe = users_with_ohe[['user'] + user_cols]\n",
    "        items_with_ohe = items_with_ohe[['item'] + item_cols]\n",
    "\n",
    "        a_users = users_with_ohe.drop('user', axis=1).values.astype(np.float32)\n",
    "        a_items = items_with_ohe.drop('item', axis=1).values.astype(np.float32)\n",
    "        a_users = np.ascontiguousarray(a_users)\n",
    "        a_items = np.ascontiguousarray(a_items)\n",
    "        index = faiss.index_factory(a_items.shape[1], \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "        index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, index)\n",
    "        index.add(a_items)\n",
    "        distances, idxs = index.search(a_users, num_items)\n",
    "        return pd.DataFrame({\n",
    "            'user': np.repeat(limited_users, num_items),\n",
    "            'item': limited_items[idxs.flatten()],\n",
    "            'ohe_distance': distances.flatten(),\n",
    "            'strategy': 'ohe_distance',\n",
    "        })\n",
    "\n",
    "\n",
    "    with timer(\"repurchase\"):\n",
    "        candidates_repurchase = create_candidates_repurchase('repurchase', transactions, target_users, week)\n",
    "    with timer(\"popular\"):\n",
    "        candidates_popular = create_candidates_popular(transactions, target_users, week, CFG.popular_weeks, CFG.popular_num_items)\n",
    "    with timer(\"item2item\"):\n",
    "        candidates_item2item = create_candidates_repurchase('item2item', transactions, target_users, week, CFG.item2item_num_items)\n",
    "    with timer(\"cooccurrence\"):\n",
    "        candidates_cooc = create_candidates_cooc(transactions, candidates_item2item, week, CFG.cooc_weeks, CFG.cooc_threshold)\n",
    "    with timer(\"same_product_code\"):\n",
    "        candidates_same_product_code = create_candidates_same_product_code(items, candidates_item2item)\n",
    "    with timer(\"ohe distance\"):\n",
    "        candidates_ohe_distance = create_candidates_ohe_distance(transactions, users, items, target_users, week, CFG.ohe_distance_num_weeks, CFG.ohe_distance_num_items)\n",
    "\n",
    "    def drop_common_user_item(candidates_target: pd.DataFrame, candidates_reference: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        candidates_targetのうちuser, itemの組がcandidates_referenceにあるものを落とす\n",
    "        \"\"\"\n",
    "        tmp = candidates_reference[['user', 'item']].reset_index(drop=True)\n",
    "        tmp['flag'] = 1\n",
    "        candidates = candidates_target.merge(tmp, on=['user', 'item'], how='left')\n",
    "        return candidates.query(\"flag != 1\").reset_index(drop=True).drop('flag', axis=1)\n",
    "\n",
    "\n",
    "    candidates_cooc = drop_common_user_item(candidates_cooc, candidates_repurchase)\n",
    "    candidates_same_product_code = drop_common_user_item(candidates_same_product_code, candidates_repurchase)\n",
    "    candidates_ohe_distance = drop_common_user_item(candidates_ohe_distance, candidates_repurchase)\n",
    "\n",
    "    candidates = [\n",
    "        candidates_repurchase,\n",
    "        candidates_popular,\n",
    "        candidates_cooc,\n",
    "        candidates_same_product_code,\n",
    "        candidates_ohe_distance,\n",
    "    ]\n",
    "    candidates = pd.concat(candidates)\n",
    "\n",
    "    print(f\"volume: {len(candidates)}\")\n",
    "    print(f\"duplicates: {len(candidates) / len(candidates[['user', 'item']].drop_duplicates())}\")\n",
    "\n",
    "    volumes = candidates.groupby('strategy').size().reset_index(name='volume').sort_values(by='volume', ascending=False).reset_index(drop=True)\n",
    "    volumes['ratio'] = volumes['volume'] / volumes['volume'].sum()\n",
    "    print(volumes)\n",
    "\n",
    "    meta_columns = [c for c in candidates.columns if c.endswith('_meta')]\n",
    "    return candidates.drop(meta_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid: week=0\n",
    "# train: week=1..CFG.train_weeks\n",
    "candidates = []\n",
    "for week in range(1+CFG.train_weeks):\n",
    "    target_users = transactions.query(\"week == @week\")['user'].unique()\n",
    "    candidates.append(create_candidates(transactions, target_users, week+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_labels(candidates: pd.DataFrame, week: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    candidatesに対してweekで指定される週のトランザクションからラベルを付与する\n",
    "    \"\"\"\n",
    "    print(f\"merge labels (week: {week})\")\n",
    "    labels = transactions[transactions['week'] == week][['user', 'item']].drop_duplicates(ignore_index=True)\n",
    "    labels['y'] = 1\n",
    "    original_positives = len(labels)\n",
    "    labels = candidates.merge(labels, on=['user', 'item'], how='left')\n",
    "    labels['y'] = labels['y'].fillna(0)\n",
    "\n",
    "    remaining_positives_total = labels[['user', 'item', 'y']].drop_duplicates(ignore_index=True)['y'].sum()\n",
    "    recall = remaining_positives_total / original_positives\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "    volumes = candidates.groupby('strategy').size().reset_index(name='volume')\n",
    "    remaining_positives = labels.groupby('strategy')['y'].sum().reset_index()\n",
    "    remaining_positives = remaining_positives.merge(volumes, on='strategy')\n",
    "    remaining_positives['recall'] = remaining_positives['y'] / original_positives\n",
    "    remaining_positives['hit_ratio'] = remaining_positives['y'] / remaining_positives['volume']\n",
    "    remaining_positives = remaining_positives.sort_values(by='y', ascending=False).reset_index(drop=True)\n",
    "    print(remaining_positives)\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(candidates)):\n",
    "    candidates[idx] = merge_labels(candidates[idx], idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_trivial_users(labels):\n",
    "    \"\"\"\n",
    "    LightGBMのxendgcやlambdarankでは正例のみや負例のみのuserは学習に無意味なのと、メトリックの計算がおかしくなるので省く\n",
    "    \"\"\"\n",
    "    bef = len(labels)\n",
    "    df = labels[labels['user'].isin(labels[['user', 'y']].drop_duplicates().groupby('user').size().reset_index(name='sz').query(\"sz==2\").user)].reset_index(drop=True)\n",
    "    aft = len(df)\n",
    "    print(f\"drop trivial queries: {bef} -> {aft}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "for idx in range(len(candidates)):\n",
    "    candidates[idx]['week'] = idx\n",
    "\n",
    "candidates_valid_all = candidates[0].copy()\n",
    "\n",
    "for idx in range(len(candidates)):\n",
    "    candidates[idx] = drop_trivial_users(candidates[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age==25でのアイテムボリューム以上になるような幅を各年齢に対して求める\n",
    "tr = transactions[['user', 'item']].merge(users[['user', 'age']], on='user')\n",
    "age_volume_threshold = len(tr.query(\"24 <= age <= 26\"))\n",
    "\n",
    "age_volumes = {age: len(tr.query(\"age == @age\")) for age in range(16, 100)}\n",
    "\n",
    "age_shifts = {}\n",
    "for age in range(16, 100):\n",
    "    for i in range(0, 100):\n",
    "        low = age - i\n",
    "        high = age + i\n",
    "        age_volume = 0\n",
    "        for j in range(low, high+1):\n",
    "            age_volume += age_volumes.get(j, 0)\n",
    "        if age_volume >= age_volume_threshold:\n",
    "            age_shifts[age] = i\n",
    "            break\n",
    "print(age_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_features(transactions: pd.DataFrame, users: pd.DataFrame, items: pd.DataFrame, candidates: pd.DataFrame, week: int, pretrain_week: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    user, itemに対して特徴を横付けする\n",
    "    week: これを含めた以前の情報は使って良い\n",
    "    \"\"\"\n",
    "    print(f\"attach features (week: {week})\")\n",
    "    n_original = len(candidates)\n",
    "    df = candidates.copy()\n",
    "\n",
    "    with timer(\"user static fetaures\"):\n",
    "        user_features = ['FN', 'Active', 'age', 'club_member_status_idx', 'fashion_news_frequency_idx']\n",
    "        df = df.merge(users[['user'] + user_features], on='user')\n",
    "\n",
    "    with timer(\"item stacic features\"):\n",
    "        item_features = [c for c in items.columns if c.endswith('idx')]\n",
    "        df = df.merge(items[['item'] + item_features], on='item')\n",
    "\n",
    "    with timer(\"user dynamic features (transactions)\"):\n",
    "        week_end = week + CFG.dynamic_feature_weeks\n",
    "        tmp = transactions.query(\"@week <= week < @week_end\").groupby('user')[['price', 'sales_channel_id']].agg(['mean', 'std'])\n",
    "        tmp.columns = ['user_' + '_'.join(a) for a in tmp.columns.to_flat_index()]\n",
    "        df = df.merge(tmp, on='user', how='left')\n",
    "\n",
    "    with timer(\"item dynamic features (transactions)\"):\n",
    "        week_end = week + CFG.dynamic_feature_weeks\n",
    "        tmp = transactions.query(\"@week <= week < @week_end\").groupby('item')[['price', 'sales_channel_id']].agg(['mean', 'std'])\n",
    "        tmp.columns = ['item_' + '_'.join(a) for a in tmp.columns.to_flat_index()]\n",
    "        df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    with timer(\"item dynamic features (user features)\"):\n",
    "        week_end = week + CFG.dynamic_feature_weeks\n",
    "        tmp = transactions.query(\"@week <= week < @week_end\").merge(users[['user', 'age']], on='user')\n",
    "        tmp = tmp.groupby('item')['age'].agg(['mean', 'std'])\n",
    "        tmp.columns = [f'age_{a}' for a in tmp.columns.to_flat_index()]\n",
    "        df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    with timer(\"item freshness features\"):\n",
    "        tmp = transactions.query(\"@week <= week\").groupby('item')['day'].min().reset_index(name='item_day_min')\n",
    "        tmp['item_day_min'] -= transactions.query(\"@week == week\")['day'].min()\n",
    "        df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    with timer(\"item volume features\"):\n",
    "        week_end = week + CFG.volume_feature_weeks\n",
    "        tmp = transactions.query(\"@week <= week < @week_end\").groupby('item').size().reset_index(name='item_volume')\n",
    "        df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    with timer(\"user freshness features\"):\n",
    "        tmp = transactions.query(\"@week <= week\").groupby('user')['day'].min().reset_index(name='user_day_min')\n",
    "        tmp['user_day_min'] -= transactions.query(\"@week == week\")['day'].min()\n",
    "        df = df.merge(tmp, on='user', how='left')\n",
    "\n",
    "    with timer(\"user volume features\"):\n",
    "        week_end = week + CFG.volume_feature_weeks\n",
    "        tmp = transactions.query(\"@week <= week < @week_end\").groupby('user').size().reset_index(name='user_volume')\n",
    "        df = df.merge(tmp, on='user', how='left')\n",
    "\n",
    "    with timer(\"user-item freshness features\"):\n",
    "        tmp = transactions.query(\"@week <= week\").groupby(['user', 'item'])['day'].min().reset_index(name='user_item_day_min')\n",
    "        tmp['user_item_day_min'] -= transactions.query(\"@week == week\")['day'].min()\n",
    "        df = df.merge(tmp, on=['item', 'user'], how='left')\n",
    "\n",
    "    with timer(\"user-item volume features\"):\n",
    "        week_end = week + CFG.volume_feature_weeks\n",
    "        tmp = transactions.query(\"@week <= week < @week_end\").groupby(['user', 'item']).size().reset_index(name='user_item_volume')\n",
    "        df = df.merge(tmp, on=['user', 'item'], how='left')\n",
    "\n",
    "    with timer(\"item age volume features\"):\n",
    "        week_end = week + CFG.volume_feature_weeks\n",
    "        tr = transactions.query(\"@week <= week < @week_end\")[['user', 'item']].merge(users[['user', 'age']], on='user')\n",
    "        item_age_volumes = []\n",
    "        for age in range(16, 100):\n",
    "            low = age - age_shifts[age]\n",
    "            high = age + age_shifts[age]\n",
    "            tmp = tr.query(\"@low <= age <= @high\").groupby('item').size().reset_index(name='age_volume')\n",
    "            tmp['age_volume'] = tmp['age_volume'].rank(ascending=False)\n",
    "            tmp['age'] = age\n",
    "            item_age_volumes.append(tmp)\n",
    "        item_age_volumes = pd.concat(item_age_volumes)\n",
    "        df = df.merge(item_age_volumes, on=['item', 'age'], how='left')\n",
    "\n",
    "    with timer(\"ohe dot products\"):\n",
    "        item_target_cols = [c for c in items.columns if c.endswith('_idx')]\n",
    "\n",
    "        items_with_ohe = pd.get_dummies(items[['item'] + item_target_cols], columns=item_target_cols)\n",
    "\n",
    "        users_with_ohe = users[['user']]\n",
    "        for c in item_target_cols:\n",
    "            tmp = pd.read_pickle(f\"artifacts/user_features/user_ohe_agg_dataset{dataset}_week{week}_{c}.pkl\")\n",
    "            assert tmp['user'].tolist() == users_with_ohe['user'].tolist()\n",
    "            tmp = tmp[['user'] + [c for c in tmp.columns if c.endswith('_mean')]]\n",
    "            tmp = tmp.drop('user', axis=1)\n",
    "            users_with_ohe = pd.concat([users_with_ohe, tmp], axis=1)\n",
    "\n",
    "        assert items_with_ohe['item'].tolist() == items['item'].tolist()\n",
    "        assert users_with_ohe['user'].tolist() == users['user'].tolist()\n",
    "\n",
    "        users_items = df[['user', 'item']].drop_duplicates().reset_index(drop=True)\n",
    "        n_split = 10\n",
    "        n_chunk = (len(users_items) + n_split - 1)// n_split\n",
    "        ohe = []\n",
    "        for i in range(0, len(users_items), n_chunk):\n",
    "            users_items_small = users_items.iloc[i:i+n_chunk].reset_index(drop=True)\n",
    "            users_small = users_items_small['user'].values\n",
    "            items_small = users_items_small['item'].values\n",
    "\n",
    "            for item_col in item_target_cols:\n",
    "                i_cols = [c for c in items_with_ohe.columns if c.startswith(item_col)]\n",
    "                u_cols = [f\"user_ohe_agg_{c}_mean\" for c in i_cols]\n",
    "                users_items_small[f'{item_col}_ohe_score'] = (items_with_ohe[i_cols].values[items_small] * users_with_ohe[u_cols].values[users_small]).sum(axis=1)\n",
    "\n",
    "            ohe_cols = [f'{col}_ohe_score' for col in item_target_cols]\n",
    "            users_items_small = users_items_small[['user', 'item'] + ohe_cols]\n",
    "\n",
    "            ohe.append(users_items_small)\n",
    "        ohe = pd.concat(ohe)\n",
    "        df = df.merge(ohe, on=['user', 'item'])\n",
    "\n",
    "    with timer(\"lfm features\"):\n",
    "        user_reps, _ = calc_embeddings('i_i', dataset, pretrain_week, 16)\n",
    "        df = df.merge(user_reps, on='user')\n",
    "\n",
    "    assert len(df) == n_original\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset_valid_all = attach_features(transactions, users, items, candidates_valid_all, 1, CFG.train_weeks+1)\n",
    "# pretrained modelの学習期間が評価時と提出時で異なるので、candidatesは残しておく\n",
    "datasets = [attach_features(transactions, users, items, candidates[idx], 1+idx, CFG.train_weeks+1) for idx in range(len(candidates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(datasets)):\n",
    "    datasets[idx]['query_group'] = datasets[idx]['week'].astype(str) + '_' + datasets[idx]['user'].astype(str)\n",
    "    datasets[idx] = datasets[idx].sort_values(by='query_group').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_train(datasets, begin, num):\n",
    "    train = pd.concat([datasets[idx] for idx in range(begin, begin+num)])\n",
    "    return train\n",
    "\n",
    "valid = datasets[0]\n",
    "train = concat_train(datasets, 1, CFG.train_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [c for c in valid.columns if c not in ['y', 'strategy', 'query_group', 'week']]\n",
    "print(feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_feature_values = [c for c in feature_columns if c.endswith('idx')]\n",
    "cat_features = [feature_columns.index(c) for c in cat_feature_values]\n",
    "print(cat_feature_values, cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_group(df):\n",
    "    def run_length_encoding(sequence):\n",
    "        comp_seq_index, = np.concatenate(([True], sequence[1:] != sequence[:-1], [True])).nonzero()\n",
    "        return sequence[comp_seq_index[:-1]], np.ediff1d(comp_seq_index)\n",
    "    users = df['user'].values\n",
    "    _, group = run_length_encoding(users)\n",
    "    return list(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.model_type == 'LightGBM':\n",
    "    group_train = get_query_group(train)\n",
    "    group_valid = get_query_group(valid)\n",
    "\n",
    "    train_dataset = lgb.Dataset(train[feature_columns], train['y'], group=group_train)\n",
    "    valid_dataset = lgb.Dataset(valid[feature_columns], valid['y'], group=group_valid, reference=train_dataset)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'xendcg',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 1e-6,\n",
    "        'num_leaves': 255,\n",
    "        'min_data_in_leaf': 100,\n",
    "        'metric': 'map',\n",
    "        'eval_at': 12,\n",
    "    }\n",
    "\n",
    "    model = lgb.train(params, train_dataset, valid_sets=[train_dataset, valid_dataset], num_boost_round=1000, callbacks=[lgb.early_stopping(20)])\n",
    "\n",
    "    lgb.plot_importance(model, importance_type='gain', figsize=(8, 16))\n",
    "\n",
    "elif CFG.model_type == 'CatBoost':\n",
    "    train_dataset = catboost.Pool(data=train[feature_columns], label=train['y'], group_id=train['query_group'], cat_features=cat_features)\n",
    "    valid_dataset = catboost.Pool(data=valid[feature_columns], label=valid['y'], group_id=valid['query_group'], cat_features=cat_features)\n",
    "\n",
    "    params = {\n",
    "        'loss_function': 'YetiRank',\n",
    "        'use_best_model': True,\n",
    "        'one_hot_max_size': 300,\n",
    "        'iterations': 1000,\n",
    "    }\n",
    "    model = catboost.CatBoost(params)\n",
    "    model.fit(train_dataset, eval_set=valid_dataset)\n",
    "\n",
    "    plt.plot(model.get_evals_result()['validation']['PFound'])\n",
    "\n",
    "    feature_importance = model.get_feature_importance(train_dataset)\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    plt.yticks(range(len(feature_columns)), np.array(feature_columns)[sorted_idx])\n",
    "    plt.barh(range(len(feature_columns)), feature_importance[sorted_idx])\n",
    "\n",
    "\n",
    "del train, valid, train_dataset, valid_dataset\n",
    "gc.collect()\n",
    "with open('output/model_for_validation.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dataset_valid_all[['user', 'item']].reset_index(drop=True)\n",
    "pred['pred'] = model.predict(dataset_valid_all[feature_columns])\n",
    "\n",
    "pred = pred.groupby(['user', 'item'])['pred'].max().reset_index()\n",
    "pred = pred.sort_values(by=['user', 'pred'], ascending=False).reset_index(drop=True).groupby('user')['item'].apply(lambda x: list(x)[:12]).reset_index()\n",
    "\n",
    "gt = transactions.query(\"week == 0\").groupby('user')['item'].apply(list).reset_index().rename(columns={'item': 'gt'})\n",
    "merged = gt.merge(pred, on='user', how='left')\n",
    "merged['item'] = merged['item'].fillna('').apply(list)\n",
    "\n",
    "merged.to_pickle(f'output/merged_{dataset}.pkl')\n",
    "dataset_valid_all.to_pickle(f'output/valid_all_{dataset}.pkl')\n",
    "\n",
    "print('mAP@12:', mapk(merged['gt'], merged['item']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [attach_features(transactions, users, items, candidates[idx], 1+idx, CFG.train_weeks) for idx in range(len(candidates))]\n",
    "\n",
    "for idx in range(len(datasets)):\n",
    "    datasets[idx]['query_group'] = datasets[idx]['week'].astype(str) + '_' + datasets[idx]['user'].astype(str)\n",
    "    datasets[idx] = datasets[idx].sort_values(by='query_group').reset_index(drop=True)\n",
    "\n",
    "train = concat_train(datasets, 0, CFG.train_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.model_type == 'LightGBM':\n",
    "    group_train = get_query_group(train)\n",
    "    train_dataset = lgb.Dataset(train[feature_columns], train['y'], group=group_train)\n",
    "\n",
    "    best_iteration = model.best_iteration\n",
    "    model = lgb.train(params, train_dataset, num_boost_round=best_iteration)\n",
    "\n",
    "    lgb.plot_importance(model, importance_type='gain', figsize=(8, 16))\n",
    "\n",
    "elif CFG.model_type == 'CatBoost':\n",
    "    train_dataset = catboost.Pool(data=train[feature_columns], label=train['y'], group_id=train['query_group'], cat_features=cat_features)\n",
    "\n",
    "    params['iterations'] = model.get_best_iteration()\n",
    "    params['use_best_model'] = False\n",
    "    model = catboost.CatBoost(params)\n",
    "    model.fit(train_dataset)\n",
    "\n",
    "    feature_importance = model.get_feature_importance(train_dataset)\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    plt.yticks(range(len(feature_columns)), np.array(feature_columns)[sorted_idx])\n",
    "    plt.barh(range(len(feature_columns)), feature_importance[sorted_idx])\n",
    "\n",
    "del train, train_dataset\n",
    "gc.collect()\n",
    "with open('output/model_for_submission.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del datasets, dataset_valid_all, candidates, candidates_valid_all\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = users['user'].values\n",
    "all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid OOM\n",
    "preds = []\n",
    "\n",
    "n_split_prediction = 10\n",
    "n_chunk = (len(all_users) + n_split_prediction - 1)// n_split_prediction\n",
    "for i in range(0, len(all_users), n_chunk):\n",
    "    print(f\"chunk: {i}\")\n",
    "    target_users = all_users[i:i+n_chunk]\n",
    "\n",
    "    candidates = create_candidates(transactions, target_users, 0)\n",
    "    candidates = attach_features(transactions, users, items, candidates, 0, CFG.train_weeks)\n",
    "\n",
    "    candidates['pred'] = model.predict(candidates[feature_columns])\n",
    "    pred = candidates.groupby(['user', 'item'])['pred'].max().reset_index()\n",
    "    pred = pred.sort_values(by=['user', 'pred'], ascending=False).reset_index(drop=True).groupby('user')['item'].apply(lambda x: list(x)[:12]).reset_index()\n",
    "    preds.append(pred)\n",
    "\n",
    "pred = pd.concat(preds).reset_index(drop=True)\n",
    "assert len(pred) == len(all_users)\n",
    "assert np.array_equal(pred['user'].values, all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plot_images(dataset, pred['item'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_user = pd.read_pickle(f\"input/{dataset}/mp_customer_id.pkl\")\n",
    "mp_item = pd.read_pickle(f\"input/{dataset}/mp_article_id.pkl\")\n",
    "\n",
    "a_user = mp_user['val'].values\n",
    "a_item = mp_item['val'].values\n",
    "\n",
    "pred['customer_id'] = pred['user'].apply(lambda x: a_user[x])\n",
    "pred['prediction'] = pred['item'].apply(lambda x: list(map(lambda y: a_item[y], x)))\n",
    "\n",
    "pred['prediction'] = pred['prediction'].apply(lambda x: ' '.join(map(str, x)))\n",
    "\n",
    "submission = pred[['customer_id', 'prediction']]\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c89b5301ada1d96cd3523f144e4a3cd3ad36f6698a61e6b8277fb627756a86c6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
