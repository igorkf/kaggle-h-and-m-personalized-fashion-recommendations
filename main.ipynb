{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import catboost\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from lightfm import LightFM\n",
    "from scipy import sparse\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from metric import apk, mapk\n",
    "from utils import plot_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_DIR = Path('artifacts')\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '100'\n",
    "\n",
    "transactions = pd.read_pickle(f\"input/{dataset}/transactions_train.pkl\")\n",
    "users = pd.read_pickle(f\"input/{dataset}/users.pkl\")\n",
    "items = pd.read_pickle(f\"input/{dataset}/items.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CFG:\n",
    "#     model_type = 'LightGBM'\n",
    "#     num_popular_items = 12\n",
    "#     train_weeks = 1\n",
    "#     repurchase_weeks = 12345\n",
    "#     repurchase_num_items = 100\n",
    "#     item2item_weeks = 12345\n",
    "#     item2item_num_items = 24\n",
    "#     cooc_weeks = 12\n",
    "#     cooc_threshold = 50\n",
    "#     dynamic_feature_weeks = 8\n",
    "#     volume_feature_weeks = 1\n",
    "\n",
    "\n",
    "# BEST\n",
    "class CFG:\n",
    "    model_type = 'CatBoost'\n",
    "    num_popular_items = 12\n",
    "    train_weeks = 3\n",
    "    repurchase_weeks = 123456\n",
    "    repurchase_num_items = 123456\n",
    "    item2item_weeks = 123456\n",
    "    item2item_num_items = 24\n",
    "    cooc_weeks = 12\n",
    "    cooc_threshold = 50\n",
    "    dynamic_feature_weeks = 8\n",
    "    volume_feature_weeks = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lfm(\n",
    "        transactions: pd.DataFrame,\n",
    "        n_user: int,\n",
    "        n_item: int,\n",
    "        week_start: int,\n",
    "        week_num: int,\n",
    "):\n",
    "    week_end = week_start + week_num\n",
    "    a = transactions.query(\"@week_start <= week < @week_end\")[['user', 'item']].drop_duplicates(ignore_index=True)\n",
    "\n",
    "    a_train = sparse.lil_matrix((n_user, n_item))\n",
    "    a_train[a['user'], a['item']] = 1\n",
    "\n",
    "    lightfm_params = {\n",
    "        'no_components': 16,\n",
    "        'learning_schedule': 'adadelta',\n",
    "        'loss': 'bpr',\n",
    "        'learning_rate': 0.005,\n",
    "    }\n",
    "\n",
    "    model = LightFM(**lightfm_params)\n",
    "    model.fit(a_train, epochs=100, num_threads=psutil.cpu_count(logical=False), verbose=True)\n",
    "    with open(ARTIFACTS_DIR / f\"lfm_{dataset}_{week_start}.pkl\", 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# train_lfm(transactions, len(users), len(items), CFG.train_weeks+1, CFG.repurchase_weeks)  # for train/valid\n",
    "# train_lfm(transactions, len(users), len(items), CFG.train_weeks, CFG.repurchase_weeks)  # for train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidates(transactions: pd.DataFrame, target_users: np.ndarray, week: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    transactions\n",
    "        original transactions (user, item, week)\n",
    "    target_users, week\n",
    "        候補生成対象のユーザー\n",
    "        weekで指定されている週の段階での情報のみから作られる\n",
    "    \"\"\"\n",
    "    print(f\"week: {week}\")\n",
    "    assert len(target_users) == len(set(target_users))\n",
    "\n",
    "    def create_candidates_repurchase(\n",
    "            strategy: str,\n",
    "            transactions: pd.DataFrame,\n",
    "            target_users: np.ndarray,\n",
    "            week_start: int,\n",
    "            num_weeks: int,\n",
    "            max_items_per_user: int) -> pd.DataFrame:\n",
    "        week_end = week_start + num_weeks\n",
    "        tr = transactions.query(\"user in @target_users and @week_start <= week < @week_end\")[['user', 'item', 'week', 'day']].drop_duplicates(ignore_index=True)\n",
    "\n",
    "        gr_day = tr.groupby(['user', 'item'])['day'].min().reset_index(name='day')\n",
    "        gr_week = tr.groupby(['user', 'item'])['week'].min().reset_index(name='week')\n",
    "        gr_volume = tr.groupby(['user', 'item']).size().reset_index(name='volume')\n",
    "\n",
    "        gr_day['day_rank'] = gr_day.groupby('user')['day'].rank()\n",
    "        gr_week['week_rank'] = gr_week.groupby('user')['week'].rank()\n",
    "        gr_volume['volume_rank'] = gr_volume.groupby('user')['volume'].rank(ascending=False)\n",
    "\n",
    "        candidates = gr_day.merge(gr_week, on=['user', 'item']).merge(gr_volume, on=['user', 'item'])\n",
    "\n",
    "        candidates['rank_meta'] = 10**9 * candidates['day_rank'] + candidates['volume_rank']\n",
    "        candidates['rank_meta'] = candidates.groupby('user')['rank_meta'].rank(method='min')\n",
    "        # item2itemに使う場合は全件使うと無駄に重くなってしまうので削る\n",
    "        # dayの小ささ, volumeの大きさの辞書順にソートして上位アイテムのみ残す\n",
    "        # 全部残したい場合はmax_items_per_userに十分大きな数を指定する\n",
    "        candidates = candidates.query(\"rank_meta <= @max_items_per_user\").reset_index(drop=True)\n",
    "\n",
    "        candidates = candidates[['user', 'item', 'week_rank', 'volume_rank', 'rank_meta']].rename(columns={'week_rank': f'{strategy}_week_rank', 'volume_rank': f'{strategy}_volume_rank'})\n",
    "\n",
    "        candidates['strategy'] = strategy\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "    def create_candidates_popular(\n",
    "            strategy: str,\n",
    "            transactions: pd.DataFrame,\n",
    "            target_users: np.ndarray,\n",
    "            week_start: int,\n",
    "            week_num: int) -> pd.DataFrame:\n",
    "        week_end = week_start + week_num\n",
    "        tr = transactions.query(\"@week_start <= week < @week_end\")[['user', 'item']].drop_duplicates(ignore_index=True)\n",
    "        popular_items = tr['item'].value_counts().index.values[:CFG.num_popular_items]\n",
    "        popular_items = pd.DataFrame({\n",
    "            'item': popular_items,\n",
    "            'rank': range(CFG.num_popular_items),\n",
    "            'crossjoinkey': 1,\n",
    "        })\n",
    "\n",
    "        candidates = pd.DataFrame({\n",
    "            'user': target_users,\n",
    "            'crossjoinkey': 1,\n",
    "        })\n",
    "\n",
    "        candidates = candidates.merge(popular_items, on='crossjoinkey').drop('crossjoinkey', axis=1)\n",
    "        candidates = candidates.rename(columns={'rank': f'{strategy}_rank'})\n",
    "\n",
    "        candidates['strategy'] = strategy\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "    def create_candidates_cooc(\n",
    "            strategy: str,\n",
    "            transactions: pd.DataFrame,\n",
    "            week_start: int,\n",
    "            week_num: int,\n",
    "            base_candidates: pd.DataFrame) -> pd.DataFrame:\n",
    "        week_end = week_start + week_num\n",
    "        # TODO: 元々バグってたので閾値とか要検討\n",
    "        tr = transactions.query(\"@week_start <= week < @week_end\")[['user', 'item', 'week']].drop_duplicates(ignore_index=True)\n",
    "        tr = tr.merge(tr.rename(columns={'item': 'item_with', 'week': 'week_with'}), on='user').query(\"item != item_with and week <= week_with\")[['item', 'item_with']].reset_index(drop=True)\n",
    "        gr_sz = tr.groupby('item').size().reset_index(name='tot')\n",
    "        gr_cnt = tr.groupby(['item', 'item_with']).size().reset_index(name='cnt')\n",
    "        item2item = gr_cnt.merge(gr_sz, on='item')\n",
    "        item2item['ratio'] = item2item['cnt'] / item2item['tot']\n",
    "        item2item = item2item.query(\"cnt > @CFG.cooc_threshold\").reset_index(drop=True)\n",
    "\n",
    "        candidates = base_candidates.merge(item2item, on='item').drop(['item', 'cnt'], axis=1).rename(columns={'item_with': 'item'})\n",
    "        base_candidates_columns = [c for c in base_candidates.columns if '_' in c]\n",
    "        base_candidates_replace = {c: f\"{strategy}_{c}\" for c in base_candidates_columns}\n",
    "        candidates = candidates.rename(columns=base_candidates_replace)\n",
    "        candidates = candidates.rename(columns={'ratio': f'{strategy}_ratio', 'tot': f'{strategy}_tot'})\n",
    "\n",
    "        candidates['strategy'] = strategy\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    def create_candidates_same_product_code(\n",
    "            strategy: str,\n",
    "            items: pd.DataFrame,\n",
    "            base_candidates: pd.DataFrame) -> pd.DataFrame:\n",
    "        item2item = items[['item', 'product_code']].merge(items[['item', 'product_code']].rename({'item': 'item_with'}, axis=1), on='product_code')[['item', 'item_with']].query(\"item != item_with\").reset_index(drop=True)\n",
    "\n",
    "        candidates = base_candidates.merge(item2item, on='item').drop('item', axis=1).rename(columns={'item_with': 'item'})\n",
    "\n",
    "        candidates['min_rank_meta'] = candidates.groupby(['user', 'item'])['rank_meta'].transform('min')\n",
    "        candidates = candidates.query(\"rank_meta == min_rank_meta\").reset_index(drop=True)\n",
    "        \n",
    "        base_candidates_columns = [c for c in base_candidates.columns if '_' in c]\n",
    "        base_candidates_replace = {c: f\"{strategy}_{c}\" for c in base_candidates_columns}\n",
    "        candidates = candidates.rename(columns=base_candidates_replace)\n",
    "\n",
    "        candidates['strategy'] = strategy\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    candidates_repurchase = create_candidates_repurchase('repurchase', transactions, target_users, week, CFG.repurchase_weeks, CFG.repurchase_num_items)\n",
    "    candidates_popular = create_candidates_popular('pop', transactions, target_users, week, 1)\n",
    "\n",
    "    candidates_item2item = create_candidates_repurchase('item2item', transactions, target_users, week, CFG.repurchase_weeks, CFG.item2item_num_items)\n",
    "    candidates_cooc = create_candidates_cooc('cooc', transactions, week, CFG.cooc_weeks, candidates_item2item)\n",
    "    candidates_same_product_code = create_candidates_same_product_code('same_product_code', items, candidates_item2item)\n",
    "\n",
    "    def drop_common_user_item(candidates_target: pd.DataFrame, candidates_reference: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        candidates_targetのうちuser, itemの組がcandidates_referenceにあるものを落とす\n",
    "        \"\"\"\n",
    "        tmp = candidates_reference[['user', 'item']].reset_index(drop=True)\n",
    "        tmp['flag'] = 1\n",
    "        candidates = candidates_target.merge(tmp, on=['user', 'item'], how='left')\n",
    "        return candidates.query(\"flag != 1\").reset_index(drop=True).drop('flag', axis=1)\n",
    "\n",
    "    candidates_cooc = drop_common_user_item(candidates_cooc, candidates_repurchase)\n",
    "    candidates_same_product_code = drop_common_user_item(candidates_same_product_code, candidates_repurchase)\n",
    "\n",
    "    candidates = [\n",
    "        candidates_repurchase,\n",
    "        candidates_popular,\n",
    "        candidates_cooc,\n",
    "        candidates_same_product_code,\n",
    "    ]\n",
    "    candidates = pd.concat(candidates)\n",
    "\n",
    "    print(f\"volume: {len(candidates)}\")\n",
    "    print(f\"duplicates: {len(candidates) / len(candidates[['user', 'item']].drop_duplicates())}\")\n",
    "\n",
    "    volumes = candidates.groupby('strategy').size().reset_index(name='volume').sort_values(by='volume', ascending=False).reset_index(drop=True)\n",
    "    volumes['ratio'] = volumes['volume'] / volumes['volume'].sum()\n",
    "    print(volumes)\n",
    "\n",
    "    meta_columns = [c for c in candidates.columns if c.endswith('_meta')]\n",
    "    return candidates.drop(meta_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid: week=0\n",
    "# train: week=1..CFG.train_weeks\n",
    "candidates = []\n",
    "for week in range(1+CFG.train_weeks):\n",
    "    target_users = transactions.query(\"week == @week\")['user'].unique()\n",
    "    candidates.append(create_candidates(transactions, target_users, week+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_labels(candidates: pd.DataFrame, week: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    candidatesに対してweekで指定される週のトランザクションからラベルを付与する\n",
    "    \"\"\"\n",
    "    print(f\"week: {week}\")\n",
    "    labels = transactions[transactions['week'] == week][['user', 'item']].drop_duplicates(ignore_index=True)\n",
    "    labels['y'] = 1\n",
    "    original_positives = len(labels)\n",
    "    labels = candidates.merge(labels, on=['user', 'item'], how='left')\n",
    "    labels['y'] = labels['y'].fillna(0)\n",
    "\n",
    "    remaining_positives_total = labels[['user', 'item', 'y']].drop_duplicates(ignore_index=True)['y'].sum()\n",
    "    recall = remaining_positives_total / original_positives\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "    volumes = candidates.groupby('strategy').size().reset_index(name='volume')\n",
    "    remaining_positives = labels.groupby('strategy')['y'].sum().reset_index()\n",
    "    remaining_positives = remaining_positives.merge(volumes, on='strategy')\n",
    "    remaining_positives['recall'] = remaining_positives['y'] / original_positives\n",
    "    remaining_positives['hit_ratio'] = remaining_positives['y'] / remaining_positives['volume']\n",
    "    remaining_positives = remaining_positives.sort_values(by='y', ascending=False).reset_index(drop=True)\n",
    "    print(remaining_positives)\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(candidates)):\n",
    "    candidates[idx] = merge_labels(candidates[idx], idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_trivial_users(labels):\n",
    "    \"\"\"\n",
    "    LightGBMのxendgcやlambdarankでは正例のみや負例のみのuserは学習に無意味なのと、メトリックの計算がおかしくなるので省く\n",
    "    \"\"\"\n",
    "    bef = len(labels)\n",
    "    df = labels[labels['user'].isin(labels[['user', 'y']].drop_duplicates().groupby('user').size().reset_index(name='sz').query(\"sz==2\").user)].reset_index(drop=True)\n",
    "    aft = len(df)\n",
    "    print(f\"drop trivial queries: {bef} -> {aft}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "for idx in range(len(candidates)):\n",
    "    candidates[idx]['week'] = idx\n",
    "\n",
    "valid_all = candidates[0].copy()\n",
    "\n",
    "for idx in range(len(candidates)):\n",
    "    candidates[idx] = drop_trivial_users(candidates[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_features(transactions: pd.DataFrame, users: pd.DataFrame, items: pd.DataFrame, candidates: pd.DataFrame, week: int, pretrain_week: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    user, itemに対して特徴を横付けする\n",
    "    week: これを含めた以前の情報は使って良い\n",
    "    \"\"\"\n",
    "    print(f\"week: {week}\")\n",
    "    n_original = len(candidates)\n",
    "    df = candidates.copy()\n",
    "\n",
    "    # user static features\n",
    "    user_features = ['FN', 'Active', 'age', 'club_member_status_idx', 'fashion_news_frequency_idx']\n",
    "    df = df.merge(users[['user'] + user_features], on='user')\n",
    "\n",
    "    # item static features\n",
    "    item_features = [c for c in items.columns if c.endswith('idx')]\n",
    "    df = df.merge(items[['item'] + item_features], on='item')\n",
    "\n",
    "    # user dynamic features (transactions)\n",
    "    week_end = week + CFG.dynamic_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('user')[['price', 'sales_channel_id']].agg(['mean', 'std'])\n",
    "    tmp.columns = ['user_' + '_'.join(a) for a in tmp.columns.to_flat_index()]\n",
    "    df = df.merge(tmp, on='user', how='left')\n",
    "\n",
    "    # item dynamic features (transactions)\n",
    "    week_end = week + CFG.dynamic_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('item')[['price', 'sales_channel_id']].agg(['mean', 'std'])\n",
    "    tmp.columns = ['item_' + '_'.join(a) for a in tmp.columns.to_flat_index()]\n",
    "    df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    # item dynamic features (user features)\n",
    "    week_end = week + CFG.dynamic_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").merge(users[['user', 'age']], on='user')\n",
    "    tmp = tmp.groupby('item')['age'].agg(['mean', 'std'])\n",
    "    tmp.columns = [f'age_{a}' for a in tmp.columns.to_flat_index()]\n",
    "    df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    # item freshness features\n",
    "    tmp = transactions.query(\"@week <= week\").groupby('item')['day'].min().reset_index(name='item_day_min')\n",
    "    tmp['item_day_min'] -= transactions.query(\"@week == week\")['day'].min()\n",
    "    df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    # item volume features\n",
    "    week_end = week + CFG.volume_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('item').size().reset_index(name='item_volume')\n",
    "    df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    # # user freshness features\n",
    "    # tmp = transactions.query(\"@week <= week\").groupby('user')['day'].min().reset_index(name='user_day_min')\n",
    "    # tmp['user_day_min'] -= transactions.query(\"@week == week\")['day'].min()\n",
    "    # df = df.merge(tmp, on='user', how='left')\n",
    "\n",
    "    # # user volume features\n",
    "    # week_end = week + CFG.volume_feature_weeks\n",
    "    # tmp = transactions.query(\"@week <= week < @week_end\").groupby('user').size().reset_index(name='user_volume')\n",
    "    # df = df.merge(tmp, on='user', how='left')\n",
    "\n",
    "    # user-item freshness features\n",
    "    tmp = transactions.query(\"@week <= week\").groupby(['user', 'item'])['day'].min().reset_index(name='user_item_day_min')\n",
    "    tmp['user_item_day_min'] -= transactions.query(\"@week == week\")['day'].min()\n",
    "\n",
    "    df = df.merge(tmp, on=['item', 'user'], how='left')\n",
    "\n",
    "    # user-item volume features\n",
    "    week_end = week + CFG.volume_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby(['user', 'item']).size().reset_index(name='user_item_volume')\n",
    "    df = df.merge(tmp, on=['user', 'item'], how='left')\n",
    "\n",
    "    # lfm features\n",
    "    lfm_path = ARTIFACTS_DIR / f\"lfm_{dataset}_{pretrain_week}.pkl\"\n",
    "\n",
    "    with open(lfm_path, 'rb') as f:\n",
    "        lfm = pickle.load(f)\n",
    "    no_components = lfm.get_params()['no_components']\n",
    "\n",
    "    # df['lfm'] = lfm.predict(df['user'].values, df['item'].values, num_threads=psutil.cpu_count(logical=False))\n",
    "\n",
    "    user_reps = np.hstack([lfm.user_embeddings, lfm.user_biases.reshape((len(users), 1))])\n",
    "    user_reps = pd.DataFrame(user_reps, columns=[f'user_rep_{i}' for i in range(no_components+1)])\n",
    "    user_reps = pd.concat([pd.DataFrame({'user': range(len(users))}), user_reps], axis=1)\n",
    "    df = df.merge(user_reps, on='user')\n",
    "\n",
    "    assert len(df) == n_original\n",
    "    return df\n",
    "\n",
    "\n",
    "valid_all = attach_features(transactions, users, items, valid_all, 1, CFG.train_weeks+1)\n",
    "# pretrained modelの学習期間が評価時と提出時で異なるので、candidatesは残しておく\n",
    "datasets = [attach_features(transactions, users, items, candidates[idx], 1+idx, CFG.train_weeks+1) for idx in range(len(candidates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(datasets)):\n",
    "    datasets[idx]['query_group'] = datasets[idx]['week'].astype(str) + '_' + datasets[idx]['user'].astype(str)\n",
    "    datasets[idx] = datasets[idx].sort_values(by='query_group').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_train(datasets, begin, num):\n",
    "    train = pd.concat([datasets[idx] for idx in range(begin, begin+num)])\n",
    "    return train\n",
    "\n",
    "valid = datasets[0]\n",
    "train = concat_train(datasets, 1, CFG.train_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [c for c in valid.columns if c not in ['y', 'strategy', 'query_group', 'week']]\n",
    "print(feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_feature_values = [c for c in feature_columns if c.endswith('idx')]\n",
    "cat_features = [feature_columns.index(c) for c in cat_feature_values]\n",
    "print(cat_feature_values, cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_group(df):\n",
    "    def run_length_encoding(sequence):\n",
    "        comp_seq_index, = np.concatenate(([True], sequence[1:] != sequence[:-1], [True])).nonzero()\n",
    "        return sequence[comp_seq_index[:-1]], np.ediff1d(comp_seq_index)\n",
    "    users = df['user'].values\n",
    "    _, group = run_length_encoding(users)\n",
    "    return list(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.model_type == 'LightGBM':\n",
    "    group_train = get_query_group(train)\n",
    "    group_valid = get_query_group(valid)\n",
    "\n",
    "    train_dataset = lgb.Dataset(train[feature_columns], train['y'], group=group_train)\n",
    "    valid_dataset = lgb.Dataset(valid[feature_columns], valid['y'], group=group_valid, reference=train_dataset)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'xendcg',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 1e-6,\n",
    "        'num_leaves': 255,\n",
    "        'min_data_in_leaf': 100,\n",
    "        'metric': 'map',\n",
    "        'eval_at': 12,\n",
    "    }\n",
    "\n",
    "    model = lgb.train(params, train_dataset, valid_sets=[train_dataset, valid_dataset], num_boost_round=1000, callbacks=[lgb.early_stopping(20)])\n",
    "\n",
    "    lgb.plot_importance(model, importance_type='gain', figsize=(8, 16))\n",
    "\n",
    "elif CFG.model_type == 'CatBoost':\n",
    "    train_pool = catboost.Pool(data=train[feature_columns], label=train['y'], group_id=train['query_group'], cat_features=cat_features)\n",
    "    valid_pool = catboost.Pool(data=valid[feature_columns], label=valid['y'], group_id=valid['query_group'], cat_features=cat_features)\n",
    "\n",
    "    params = {\n",
    "        'loss_function': 'YetiRank',\n",
    "        'use_best_model': True,\n",
    "        'one_hot_max_size': 300,\n",
    "    }\n",
    "    model = catboost.CatBoost(params)\n",
    "    model.fit(train_pool, eval_set=valid_pool)\n",
    "\n",
    "    feature_importance = model.get_feature_importance(train_pool)\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    plt.yticks(range(len(feature_columns)), np.array(feature_columns)[sorted_idx])\n",
    "    plt.barh(range(len(feature_columns)), feature_importance[sorted_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_all['pred'] = model.predict(valid_all[feature_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = valid_all.groupby(['user', 'item'])['pred'].max().reset_index()\n",
    "pred = pred.sort_values(by=['user', 'pred'], ascending=False).reset_index(drop=True).groupby('user')['item'].apply(lambda x: list(x)[:12]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = transactions.query(\"week == 0\").groupby('user')['item'].apply(list).reset_index().rename(columns={'item': 'gt'})\n",
    "merged = gt.merge(pred, on='user', how='left')\n",
    "merged['item'] = merged['item'].fillna('').apply(list)\n",
    "\n",
    "print('mAP@12:', mapk(merged['gt'], merged['item']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [attach_features(transactions, users, items, candidates[idx], 1+idx, CFG.train_weeks) for idx in range(len(candidates))]\n",
    "\n",
    "for idx in range(len(datasets)):\n",
    "    datasets[idx]['query_group'] = datasets[idx]['week'].astype(str) + '_' + datasets[idx]['user'].astype(str)\n",
    "    datasets[idx] = datasets[idx].sort_values(by='query_group').reset_index(drop=True)\n",
    "\n",
    "train = concat_train(datasets, 0, CFG.train_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.model_type == 'LightGBM':\n",
    "    group_train = get_query_group(train)\n",
    "    train_dataset = lgb.Dataset(train[feature_columns], train['y'], group=group_train)\n",
    "\n",
    "    best_iteration = model.best_iteration\n",
    "    model = lgb.train(params, train_dataset, num_boost_round=best_iteration)\n",
    "\n",
    "    lgb.plot_importance(model, importance_type='gain', figsize=(8, 16))\n",
    "\n",
    "elif CFG.model_type == 'CatBoost':\n",
    "    train_pool = catboost.Pool(data=train[feature_columns], label=train['y'], group_id=train['query_group'], cat_features=cat_features)\n",
    "\n",
    "    params['iterations'] = model.get_best_iteration()\n",
    "    params['use_best_model'] = False\n",
    "    model = catboost.CatBoost(params)\n",
    "    model.fit(train_pool)\n",
    "\n",
    "    feature_importance = model.get_feature_importance(train_pool)\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    plt.yticks(range(len(feature_columns)), np.array(feature_columns)[sorted_idx])\n",
    "    plt.barh(range(len(feature_columns)), feature_importance[sorted_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = users['user'].values\n",
    "all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid OOM\n",
    "preds = []\n",
    "\n",
    "n_split_prediction = 10\n",
    "n_chunk = (len(all_users) + n_split_prediction - 1)// n_split_prediction\n",
    "for i in range(0, len(all_users), n_chunk):\n",
    "    target_users = all_users[i:i+n_chunk]\n",
    "\n",
    "    candidates = create_candidates(transactions, target_users, 0)\n",
    "    candidates = attach_features(transactions, users, items, candidates, 0, CFG.train_weeks)\n",
    "\n",
    "    candidates['pred'] = model.predict(candidates[feature_columns])\n",
    "    pred = candidates.groupby(['user', 'item'])['pred'].max().reset_index()\n",
    "    pred = pred.sort_values(by=['user', 'pred'], ascending=False).reset_index(drop=True).groupby('user')['item'].apply(lambda x: list(x)[:12]).reset_index()\n",
    "    preds.append(pred)\n",
    "\n",
    "pred = pd.concat(preds).reset_index(drop=True)\n",
    "assert len(pred) == len(all_users)\n",
    "assert np.array_equal(pred['user'].values, all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plot_images(dataset, pred['item'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_user = pd.read_pickle(f\"input/{dataset}/mp_customer_id.pkl\")\n",
    "mp_item = pd.read_pickle(f\"input/{dataset}/mp_article_id.pkl\")\n",
    "\n",
    "a_user = mp_user['val'].values\n",
    "a_item = mp_item['val'].values\n",
    "\n",
    "pred['customer_id'] = pred['user'].apply(lambda x: a_user[x])\n",
    "pred['prediction'] = pred['item'].apply(lambda x: list(map(lambda y: a_item[y], x)))\n",
    "\n",
    "pred['prediction'] = pred['prediction'].apply(lambda x: ' '.join(map(str, x)))\n",
    "\n",
    "submission = pred[['customer_id', 'prediction']]\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c89b5301ada1d96cd3523f144e4a3cd3ad36f6698a61e6b8277fb627756a86c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('3.9.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
